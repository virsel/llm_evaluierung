# yaml-language-server: $schema=https://promptfoo.dev/config-schema.json
description: 'Tests mit LLM embeddings und judge'


# Global provider configuration
providers:
  - file://retrieve_phi.py

defaultTest:
  options:
    provider:
      text: openai:chat
      embedding: ollama:embeddings:mxbai-embed-large
  assert:
    - type: answer-relevance # ensure that LLM output is related to original query
      threshold: 0.9


prompts:
 - &basic_qanda |
   Sie sind ein Assistent für Fragen und Antworten.
   
   Kontext: {{context}}
   Frage: {{query}}
   
   Beantworten Sie die Frage präzise basierend auf dem gegebenen Kontext.
   Verwenden Sie nur Informationen, die im Kontext enthalten sind.

tests: file://data/tests_qanda_lazy_context.csv