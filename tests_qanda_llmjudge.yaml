# yaml-language-server: $schema=https://promptfoo.dev/config-schema.json
description: 'Tests mit LLM judge'


# Global provider configuration
providers:
  - file://retrieve_phi.py

defaultTest:
  options:
    provider:
      text: ollama:llama3.2:latest
  # vars:
  #   context: file://retrieve_context_lazy.py 
  assert:
    - type: factuality
      value: "{{fact}}"


prompts:
 - &basic_qanda |
   Sie sind ein Assistent für Fragen und Antworten.
   
   Kontext: {{context}}
   Frage: {{query}}
   
   Beantworten Sie die Frage präzise basierend auf dem gegebenen Kontext.
   Verwenden Sie nur Informationen, die im Kontext enthalten sind.

tests: file://data/tests_qanda_lazy_context.csv